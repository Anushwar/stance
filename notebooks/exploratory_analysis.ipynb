{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Stance Detection\n",
    "\n",
    "Understanding the data before building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_df.columns = train_df.columns.str.replace('\\ufeff', '')\n",
    "test_df.columns = test_df.columns.str.replace('\\ufeff', '')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Each Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stance in ['FAVOR', 'AGAINST', 'NONE']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{stance} Examples\")\n",
    "    print('='*80)\n",
    "    samples = train_df[train_df['Stance'] == stance].sample(n=3, random_state=42)\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\\n{row['Target']}\")\n",
    "        print(f\"{row['Tweet']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Finding: Stance ≠ Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = pd.crosstab(train_df['Stance'], train_df['Sentiment'], normalize='index') * 100\n",
    "\n",
    "print(\"Sentiment Distribution within Each Stance (%)\")\n",
    "print(confusion.round(1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "confusion.plot(kind='bar', ax=ax, stacked=True)\n",
    "ax.set_title('Stance vs Sentiment Mismatch', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Stance')\n",
    "ax.set_ylabel('Percentage')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "against_positive = len(train_df[(train_df['Stance'] == 'AGAINST') & (train_df['Sentiment'] == 'POSITIVE')])\n",
    "total_against = len(train_df[train_df['Stance'] == 'AGAINST'])\n",
    "print(f\"\\n{against_positive}/{total_against} ({against_positive/total_against*100:.1f}%) AGAINST tweets have POSITIVE sentiment\")\n",
    "print(\"Sentiment analysis alone will fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGAINST + POSITIVE Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = train_df[(train_df['Stance'] == 'AGAINST') & (train_df['Sentiment'] == 'POSITIVE')].sample(n=10, random_state=42)\n",
    "\n",
    "print(\"AGAINST Stance + POSITIVE Sentiment:\")\n",
    "for idx, row in mismatch.iterrows():\n",
    "    print(f\"\\n{row['Target']}\")\n",
    "    print(f\"{row['Tweet']}\")\n",
    "    print(f\"Opinion: {row['Opinion towards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_counts = train_df['Stance'].value_counts()\n",
    "print(\"Training Set:\")\n",
    "for stance, count in stance_counts.items():\n",
    "    print(f\"  {stance:8s}: {count:4d} ({count/len(train_df)*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nImbalance Ratio: {stance_counts.max() / stance_counts.min():.2f}:1\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "stance_counts.plot(kind='bar', ax=axes[0], color=['#e74c3c', '#3498db', '#95a5a6'])\n",
    "axes[0].set_title('Training Set', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(stance_counts):\n",
    "    axes[0].text(i, v + 30, f'{v}\\n({v/len(train_df)*100:.1f}%)', ha='center', fontweight='bold')\n",
    "\n",
    "test_stance_counts = test_df['Stance'].value_counts()\n",
    "test_stance_counts.plot(kind='bar', ax=axes[1], color=['#e74c3c', '#3498db', '#95a5a6'])\n",
    "axes[1].set_title('Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "for i, v in enumerate(test_stance_counts):\n",
    "    axes[1].text(i, v + 15, f'{v}\\n({v/len(test_df)*100:.1f}%)', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass imbalance requires SMOTE or class weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance by Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_by_target = pd.crosstab(train_df['Target'], train_df['Stance'], normalize='index') * 100\n",
    "print(\"Stance Distribution by Target (%)\")\n",
    "print(stance_by_target.round(1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "stance_by_target.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db', '#95a5a6'])\n",
    "ax.set_title('Stance by Target', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Percentage')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Stance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClimate Change is mostly FAVOR - target is a strong feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words per Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return [w for w in text.split() if len(w) > 2]\n",
    "\n",
    "for stance in ['AGAINST', 'FAVOR', 'NONE']:\n",
    "    texts = train_df[train_df['Stance'] == stance]['Tweet']\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(get_words(text))\n",
    "    \n",
    "    word_freq = Counter(all_words)\n",
    "    print(f\"\\n{stance} - Top 20 Words:\")\n",
    "    for word, count in word_freq.most_common(20):\n",
    "        print(f\"  {word:20s}: {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinctive Words per Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_words = {}\n",
    "for stance in ['AGAINST', 'FAVOR', 'NONE']:\n",
    "    texts = train_df[train_df['Stance'] == stance]['Tweet']\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(get_words(text))\n",
    "    stance_words[stance] = Counter(all_words)\n",
    "\n",
    "def get_distinctive_words(target_stance, other_stances, top_n=15):\n",
    "    target_freq = stance_words[target_stance]\n",
    "    ratios = {}\n",
    "    \n",
    "    for word in target_freq:\n",
    "        if target_freq[word] < 10:\n",
    "            continue\n",
    "        \n",
    "        other_freq = sum([stance_words[s][word] for s in other_stances])\n",
    "        if other_freq == 0:\n",
    "            ratio = float('inf')\n",
    "        else:\n",
    "            ratio = target_freq[word] / (other_freq / len(other_stances))\n",
    "        ratios[word] = ratio\n",
    "    \n",
    "    return sorted(ratios.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(\"Distinctive Words per Stance:\")\n",
    "for stance in ['AGAINST', 'FAVOR', 'NONE']:\n",
    "    others = [s for s in ['AGAINST', 'FAVOR', 'NONE'] if s != stance]\n",
    "    distinctive = get_distinctive_words(stance, others, top_n=15)\n",
    "    \n",
    "    print(f\"\\n{stance}:\")\n",
    "    for word, ratio in distinctive:\n",
    "        count = stance_words[stance][word]\n",
    "        print(f\"  {word:20s}: {count:4d} ({ratio:.1f}x more frequent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NONE Stance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_samples = train_df[train_df['Stance'] == 'NONE'].sample(n=15, random_state=42)\n",
    "\n",
    "print(\"NONE Stance Examples:\")\n",
    "for idx, row in none_samples.iterrows():\n",
    "    print(f\"\\n{row['Target']}\")\n",
    "    print(f\"{row['Tweet']}\")\n",
    "    print(f\"Sentiment: {row['Sentiment']}, Opinion: {row['Opinion towards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "1. **Stance ≠ Sentiment** - 30.9% of AGAINST tweets have POSITIVE sentiment\n",
    "2. **Class imbalance** - 2.26:1 ratio requires SMOTE\n",
    "3. **Target matters** - Climate Change is mostly FAVOR\n",
    "4. **NONE is hardest** - Ambiguous by nature\n",
    "5. **Limited data** - Only 2,647 training samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
